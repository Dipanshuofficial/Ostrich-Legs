{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶Ö Ostrich-Legs: Colab Worker Node\n",
        "Run the cells below in sequence (or click **Runtime > Run all**) to connect this Colab instance to your compute swarm."
      ],
      "metadata": {
        "id": "header-md"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-deps"
      },
      "outputs": [],
      "source": [
        "# 1. INSTALL DEPENDENCIES\n",
        "!pip install python-socketio[client] requests torch numpy --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports-gpu"
      },
      "outputs": [],
      "source": [
        "# 2. IMPORTS & HARDWARE DETECTION\n",
        "import sys, os, time, uuid, threading, multiprocessing\n",
        "import numpy as np\n",
        "import socketio\n",
        "\n",
        "HAS_GPU = False\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        HAS_GPU = True\n",
        "        print(f\"üöÄ GPU DETECTED: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è GPU not found. Falling back to CPU compute.\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è PyTorch not installed. Falling back to CPU compute.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config-params"
      },
      "outputs": [],
      "source": [
        "# 3. SWARM CONFIGURATION\n",
        "# @markdown Enter your server details and swarm invite code below:\n",
        "\n",
        "SERVER_URL = \"https://your-tunnel-url.trycloudflare.com\" # @param {type:\"string\"}\n",
        "JOIN_CODE = \"\" # @param {type:\"string\"}\n",
        "DEVICE_NAME = \"Colab-Titan-Node\" # @param {type:\"string\"}\n",
        "\n",
        "# --- IDENTITY MANAGEMENT ---\n",
        "id_file = \"device_identity.txt\"\n",
        "if os.path.exists(id_file):\n",
        "    with open(id_file, \"r\") as f:\n",
        "        DEVICE_ID = f.read().strip()\n",
        "else:\n",
        "    DEVICE_ID = f\"node-colab-{str(uuid.uuid4())[:8]}\"\n",
        "    with open(id_file, \"w\") as f:\n",
        "        f.write(DEVICE_ID)\n",
        "\n",
        "print(f\"üÜî Identity Loaded: {DEVICE_ID} ({DEVICE_NAME})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "engine-logic"
      },
      "outputs": [],
      "source": [
        "# 4. KERNELS & SOCKET PROTOCOLS\n",
        "sio = socketio.Client(reconnection=True, reconnection_delay=5)\n",
        "is_working = False\n",
        "last_work_time = time.time()\n",
        "\n",
        "def run_stress_test(iterations):\n",
        "    start = time.time()\n",
        "    count = int(iterations or 100000)\n",
        "    x = np.random.rand(int(count/100))\n",
        "    np.sin(x) * np.sqrt(x)\n",
        "    return float(time.time() - start)\n",
        "\n",
        "def run_matrix_mul(data):\n",
        "    start = time.time()\n",
        "    size = int(data.get('size', 1024))\n",
        "    if HAS_GPU:\n",
        "        a = torch.rand(size, size, device='cuda')\n",
        "        b = torch.rand(size, size, device='cuda')\n",
        "        torch.matmul(a, b)\n",
        "        torch.cuda.synchronize()\n",
        "    else:\n",
        "        a = np.random.rand(size, size)\n",
        "        b = np.random.rand(size, size)\n",
        "        np.dot(a, b)\n",
        "    return float(time.time() - start)\n",
        "\n",
        "def process_job(job):\n",
        "    global last_work_time\n",
        "    if not sio.connected: return False\n",
        "\n",
        "    job_id = job['id']\n",
        "    job_type = job['type']\n",
        "    \n",
        "    try:\n",
        "        duration = 0\n",
        "        if job_type == 'MATH_STRESS':\n",
        "            duration = run_stress_test(job.get('data', {}).get('iterations', 100000))\n",
        "        elif job_type == 'MAT_MUL':\n",
        "            duration = run_matrix_mul(job.get('data', {}))\n",
        "\n",
        "        last_work_time = time.time()\n",
        "        sio.emit('job:complete', {\n",
        "            'chunkId': job_id,\n",
        "            'workerId': DEVICE_ID,\n",
        "            'result': 'SUCCESS',\n",
        "            'durationMs': duration * 1000\n",
        "        })\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Job Error: {e}\")\n",
        "        sio.emit('job:complete', {\n",
        "            'chunkId': job_id,\n",
        "            'workerId': DEVICE_ID,\n",
        "            'error': str(e)\n",
        "        })\n",
        "        return False\n",
        "\n",
        "@sio.event\n",
        "def connect():\n",
        "    print(f\"\\nüü¢ Linked to Swarm! Registering as {DEVICE_NAME}...\")\n",
        "    sio.emit('device:register', {\n",
        "        'name': DEVICE_NAME,\n",
        "        'capabilities': {\n",
        "            'cpuCores': multiprocessing.cpu_count(),\n",
        "            'memoryGB': 16,\n",
        "            'gpuAvailable': HAS_GPU,\n",
        "            'gpuName': torch.cuda.get_device_name(0) if HAS_GPU else 'None'\n",
        "        }\n",
        "    })\n",
        "\n",
        "@sio.on('job:batch')\n",
        "def on_batch(jobs):\n",
        "    global is_working\n",
        "    is_working = True\n",
        "    print(f\"\\rüì¶ Processing Batch: {len(jobs)} jobs | GPU: {'ON' if HAS_GPU else 'OFF'}\", end=\"\")\n",
        "    for job in jobs:\n",
        "        if not sio.connected: break\n",
        "        process_job(job)\n",
        "    is_working = False\n",
        "    if sio.connected:\n",
        "        sio.emit('job:request_batch')\n",
        "\n",
        "@sio.on('cmd:run_benchmark')\n",
        "def on_benchmark():\n",
        "    print(\"\\nüöÄ Running Operations Benchmark...\", end=\"\")\n",
        "    try:\n",
        "        start = time.time()\n",
        "        if HAS_GPU:\n",
        "            a = torch.rand(2000, 2000, device='cuda')\n",
        "            b = torch.rand(2000, 2000, device='cuda')\n",
        "            torch.matmul(a, b)\n",
        "            torch.cuda.synchronize()\n",
        "            duration = time.time() - start\n",
        "            score = int(5000000000 / (duration + 0.00001))\n",
        "        else:\n",
        "            run_stress_test(5000000)\n",
        "            duration = time.time() - start\n",
        "            score = int(5000000 / (duration + 0.00001))\n",
        "            \n",
        "        print(f\" Score: {score:,} OPS/s\")\n",
        "        sio.emit('benchmark:result', {'score': score})\n",
        "    except Exception as e:\n",
        "        print(f\" Benchmark Failed: {e}\")\n",
        "\n",
        "@sio.event\n",
        "def disconnect():\n",
        "    print(\"\\n‚≠ï Connection lost. Reconnecting...\")\n",
        "\n",
        "def poller_loop():\n",
        "    while True:\n",
        "        try:\n",
        "            if sio.connected and not is_working and (time.time() - last_work_time > 1.0):\n",
        "                sio.emit('job:request_batch')\n",
        "            if sio.connected:\n",
        "                sio.emit('heartbeat', {'lastInteraction': time.time() * 1000})\n",
        "            time.sleep(2.0)\n",
        "        except:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-execution"
      },
      "outputs": [],
      "source": [
        "# 5. EXECUTION LAYER\n",
        "def main():\n",
        "    print(\"\\nüåê Booting Ostrich-Legs Colab Worker...\")\n",
        "    \n",
        "    # Format Auth Query per Phase 2 Server Spec\n",
        "    auth_url = f\"{SERVER_URL}?persistentId={DEVICE_ID}\"\n",
        "    \n",
        "    t = threading.Thread(target=poller_loop, daemon=True)\n",
        "    t.start()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if not sio.connected:\n",
        "                print(f\"üîå Dialing Master at {SERVER_URL}...\")\n",
        "                # Send the join code token in the auth payload\n",
        "                sio.connect(\n",
        "                    auth_url, \n",
        "                    auth={'token': JOIN_CODE}, \n",
        "                    transports=['websocket', 'polling']\n",
        "                )\n",
        "                sio.wait()\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüõë Graceful Shutdown Initiated.\")\n",
        "            if sio.connected: sio.disconnect()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Connection Retry Error: {e}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}
